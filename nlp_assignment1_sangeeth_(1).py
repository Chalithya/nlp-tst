# -*- coding: utf-8 -*-
"""NLP_Assignment1_sangeeth (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aw-Q0eRka-MpF_-0TpUzDLYwzHrUwuZp
"""

pip install nltk

pip install emoji

pip install spacy

!pip install negspacy

!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import seaborn as sns
import nltk
from tqdm import tqdm
tqdm.pandas()

from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc

import re
import emoji
import multiprocessing

from multiprocessing import Pool

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os
import warnings


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore')

import tensorflow as tf

# !gcloud config set project ml-projects-460616

import os
from google.cloud import storage

from google.cloud import storage
import os

bucket_name="yelp-sentiment-analysis"
csv_file="Sentiment_Data.csv"


local_dir = "./Twitter_sentiment_data"
os.makedirs(local_dir, exist_ok=True)

client = storage.Client()
bucket = client.bucket(bucket_name)


blob = bucket.blob("Sentiment_Data.csv")
local_path = os.path.join(local_dir, "Sentiment_Data.csv")
blob.download_to_filename(local_path)
print(f"Downloaded to {local_path}")

tweets = pd.read_csv("Twitter_sentiment_data/Sentiment_Data.csv", encoding="ISO-8859-1")

tweets.head()

tweets.shape

tweets['Sentiment'].value_counts()

"""# Removing duplicate tweets"""

sum(tweets.duplicated())

tweets.drop_duplicates(inplace=True)
sum(tweets.duplicated())

"""# Removing null tweets"""

tweets.isnull().sum()

# Removing rows where Tweet is null
tweets = tweets.dropna(subset=['Tweet']).reset_index(drop=True)
print("Remaining rows:", len(tweets))

tweets.isnull().sum()

tweets["Sentiment"].value_counts()

"""# Sampling

I am comnining mild positive to positive and taking 20k from mild positive to be able to separate from others and 10k from strong positive. Similarly for negative class as well. Finally, I am combining them to classify them into only 3 classes which are positive, negative, and neutral. This will be easy to develop. Later on, we will predict on 5 classes for more detailed.
"""

# Sampling
mild_pos = tweets[tweets['Sentiment'] == 'Mild_Pos'].sample(n=20000, random_state=42)
strong_pos = tweets[tweets['Sentiment'] == 'Strong_Pos'].sample(n=15000, random_state=42)

mild_neg = tweets[tweets['Sentiment'] == 'Mild_Neg'].sample(n=20000, random_state=42)
strong_neg = tweets[tweets['Sentiment'] == 'Strong_Neg'].sample(n=15000, random_state=42)

neutral = tweets[tweets['Sentiment'] == 'Neutral'].sample(n=350000, random_state=42)

# combing them
sampled_df = pd.concat([mild_pos, strong_pos, mild_neg, strong_neg, neutral], axis=0).reset_index(drop=True)


print(sampled_df['Sentiment'].value_counts())

sampled_df.head()

# combining them to make  3 classes
def simplify_sentiment(sent):
    if sent in ['Mild_Pos', 'Strong_Pos']:
        return 'Positive'
    elif sent in ['Mild_Neg', 'Strong_Neg']:
        return 'Negative'
    else:
        return 'Neutral'

sampled_df['Sentiment'] = sampled_df['Sentiment'].apply(simplify_sentiment)


print(sampled_df['Sentiment'].value_counts())

sampled_df.head()

"""# Dictionary of Slang words from Github"""

slang_dict  = {
    "$" : " dollar ",
    "â‚¬" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk",
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart",
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet",
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously",
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}

"""# Abbrevations"""

with open("abbreviations_eng.csv", encoding="ISO-8859-1") as f:
    for i, line in enumerate(f.readlines()[:20]):
        print(f"{i+1}: {line}")

# Loading CSV using semicolon delimiter and no header
abb = pd.read_csv("abbreviations_eng.csv",
                  encoding="ISO-8859-1",
                  delimiter=";",
                  header=None,
                  names=["Index", "abbr", "full"])


abb = abb.drop(columns=["Index"])


print(abb.head())

abbr_dict = dict(
    zip(
        abb['abbr'].astype(str).str.strip(),
        abb['full'].astype(str).str.strip()
    )
)

import re
import emoji
import spacy



def expand_terms(text, mapping_dict):
    count = 0
    for short, expanded in mapping_dict.items():
        pattern = r'\b' + re.escape(short) + r'\b'
        matches = len(re.findall(pattern, text))
        if matches > 0:
            text = re.sub(pattern, expanded, text)
            count += matches
    return text, count

def clean_text(text):
    if not isinstance(text, str):
        return "", 0, 0, 0

    text = text.lower()

    # Emoji handling
    emoji_matches = re.findall(r':[a-z_]+:', emoji.demojize(text))
    emoji_count = len(emoji_matches)
    text = emoji.demojize(text)
    text = text.replace(":", " ").replace("_", " ")

    # Slang handling
    text, slang_count = expand_terms(text, slang_dict)

    # Abbreviation handling
    text, abbr_count = expand_terms(text, abbr_dict)

    # Regex cleaning
    text = re.sub(r"http\S+|www.\S+", "", text)     # URLs
    text = re.sub(r"@\w+", "", text)                # Mentions
    text = re.sub(r"#\w+", "", text)                # Hashtags
    text = re.sub(r"&\w+;", "", text)               # HTML entities
    text = re.sub(r"[^a-z\s]", "", text)            # Non-letter characters
    text = re.sub(r"\s+", " ", text).strip()        # Whitespace



    return text, emoji_count, slang_count, abbr_count

from joblib import Parallel, delayed
from tqdm.notebook import tqdm

def parallel_clean_text_joblib(text_list, func, n_jobs=-1):
    results = Parallel(n_jobs=n_jobs)(
        delayed(func)(text) for text in tqdm(text_list)
    )
    return results

def clean_text_chunk(chunk):
    return chunk.apply(clean_text)

# Apply cleaning in parallel using joblib
results = parallel_clean_text_joblib(sampled_df['Tweet'], clean_text)

# Unpacking the tuple result into new columns in sampled_df
sampled_df['Clean_Tweet'] = [r[0] for r in results]
sampled_df['Emoji_Count'] = [r[1] for r in results]
sampled_df['Slang_Count'] = [r[2] for r in results]
sampled_df['Abbr_Count'] = [r[3] for r in results]

sampled_df.head()

blank_rows = sampled_df[sampled_df['Clean_Tweet'] == ""][['Tweet']]
print("tweets cleaned to empty:")
print(blank_rows)

"""# Removing hashtags and mentions for now."""

# Dropping rows where Clean_Tweet is empty or null
sampled_df = sampled_df[~sampled_df['Clean_Tweet'].isnull()]
sampled_df = sampled_df[sampled_df['Clean_Tweet'].str.strip() != ""]


sampled_df.reset_index(drop=True, inplace=True)

sampled_df.isnull().sum()

# import csv

# sampled_df[['Clean_Tweet', 'Sentiment']].to_csv(
#     "Twitter_sentiment_data/processed_data_without_negation.csv",
#     index=False,
#     encoding='utf-8'
# )



data_nonegate=sampled_df.copy()



"""# Negation handling"""

import spacy
from tqdm.notebook import tqdm

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

def handle_negation(text):
    doc = nlp(text)
    negated = {token.head.i for token in doc if token.dep_ == "neg"}

    return " ".join(
        ["NOT_" + token.text if i in negated else token.text for i, token in enumerate(doc)]
    )


tqdm.pandas(desc="Applying negation to Clean_Tweet")


sampled_df["Negated_Tweet"] = sampled_df["Clean_Tweet"].progress_apply(handle_negation)

sampled_df.isnull().sum()

negate_tweets=sampled_df.copy()

# from nltk.corpus import stopwords
# import nltk

# #
# nltk.download('stopwords')

"""# Removing stop words and Lemmatization"""

tqdm.pandas()


def preprocess_negated_text(text):
    if not isinstance(text, str):
        return ""

    tokens = text.split()
    protected_tokens = []

    for token in tokens:
        if token.startswith("NOT_"):
            protected_tokens.append(token.lower())
        else:
            doc = nlp(token)
            for word in doc:
                lemma = word.lemma_.lower()
                if lemma not in stop_words and lemma.isalpha():
                    protected_tokens.append(lemma)

    return " ".join(protected_tokens)


sampled_df['Processed_Tweet'] = sampled_df['Negated_Tweet'].progress_apply(preprocess_negated_text)

sampled_df

# sampled_df.to_csv("Twitter_sentiment_data/Full_processed_tweets.csv",index=False)

print("Total Emojis Handled:", sampled_df['Emoji_Count'].sum())
print("Total Slangs Handled:", sampled_df['Slang_Count'].sum())
print("Total Abbreviations Handled:", sampled_df['Abbr_Count'].sum())

#  sampled_df=pd.read_csv("Twitter_sentiment_data/processed_tweets_with_negation.csv")
# sampled_df.head()

#sampled_df.isnull().sum()

"""# Summary of Preprocessing Steps Before Splitting the Data


1. Dataset Sampling:
   - Selected 105,000 tweets:
     - 20k Mild_Pos, 15k Strong_Pos
     - 20k Mild_Neg, 15k Strong_Neg
     - 35k Neutral

2. Cleaning Pipeline:
   - Lowercased all text
   - Emoji Handling:
     - Converted emojis to text using `emoji.demojize`
     - Counted emoji occurrences
   - Slang & Abbreviation Expansion:
     - Used `slang_dict` and `abbr_dict` to expand informal terms
     - Counted replacements
   - Regex Cleaning:
     - Removed URLs, mentions (@), hashtags (#), HTML entities, special characters
     - Normalized whitespace
     
Total Emojis Handled: 7487

Total Slangs Handled: 7350

Total Abbreviations Handled: 42


3. Negation Handling:
   - Used spaCy to identify negated words
   - Prefixed negated tokens with `NOT_`

4. Post-Cleaning Fixes:
   - Detected and removed ~2,100 tweets that became empty after cleaning (which are tweets with hastags and mentions)
   - Final dataset contains only non-empty, clean tweets

"""

# sampled_df=pd.read_csv("Twitter_sentiment_data/Full_processed_tweets.csv")

sampled_df.isnull().sum()

sampled_df.dropna(subset=['Processed_Tweet'], inplace=True)
sampled_df.reset_index(drop=True, inplace=True)

sampled_df.isnull().sum()

"""# Label Encoding Sentiment column

Negative - 0
Neutral - 1
Positive - 2

"""

from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
sampled_df['Sentiment'] = label_encoder.fit_transform(sampled_df['Sentiment'])


print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

sampled_df.head()

sampled_df.isnull().sum()

"""# Splitting Data."""

from sklearn.model_selection import train_test_split

X = sampled_df['Processed_Tweet']
y = sampled_df['Sentiment']

# Splitting into train (70%), validation (15%), and test (15%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)


print("Training set size   :", len(X_train))
print("Validation set size :", len(X_val))
print("Test set size       :", len(X_test))

"""# Tokenization and padding"""

# initialize Tokenizer
vocab_size = 10000
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")

# Fittin on training data only
tokenizer.fit_on_texts(X_train)

#Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Determine max length and pad all sequences
maxlen = max([len(seq) for seq in X_train_seq])
X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post', truncating='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post', truncating='post')


print("X_train_pad shape:", X_train_pad.shape)
print("X_val_pad shape  :", X_val_pad.shape)
print("X_test_pad shape :", X_test_pad.shape)

X_train_pad

"""# Deep Bi-directional GRU - Model 1"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout

# defining model
embedding_dim = 100
hidden_units = 64

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(maxlen,)),

    # First Bidirectional GRU layer (returns sequences to stack next layer)
    Bidirectional(GRU(units=hidden_units, return_sequences=True)),
    Dropout(0.3),

    # Second Bidirectional GRU layer
    Bidirectional(GRU(units=hidden_units, return_sequences=False)),
    Dropout(0.3),

    # Dense layers
    Dense(64, activation='relu'),
    Dropout(0.3),

    # Output layer
    Dense(3, activation='softmax')  # for Positive, Neutral, Negative
])


model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

"""# Model Training"""

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)


history = model.fit(
    X_train_pad, y_train,
    epochs=10,
    batch_size=128,
    validation_data=(X_val_pad, y_val),
    callbacks=[early_stop, checkpoint],
    verbose=1
)

from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc


# results dataframe
results_df_multi = pd.DataFrame(columns=["Set", "Accuracy", "F1_macro", "ROC_AUC_macro"])

def evaluate_multiclass_model(set_name, model, X_pad, y_true, class_names=None):
    global results_df_multi

    # probabilities and predicted classes
    y_probs = model.predict(X_pad)
    y_pred = np.argmax(y_probs, axis=1)

    # metrics
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="macro")
    roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovo', average='macro')

    #Results
    results_df_multi = pd.concat([
        results_df_multi,
        pd.DataFrame([{
            "Set": set_name,
            "Accuracy": accuracy,
            "F1_macro": f1,
            "ROC_AUC_macro": roc_auc
        }])
    ], ignore_index=True)

    # classification report
    print(f"Classification Report ({set_name}):\n")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # Confusion matrix
    print(f"Confusion Matrix ({set_name}):")
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title(f"{set_name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve (One-vs-rest)
    print(f"ROC-AUC Curve ({set_name}):")
    y_true_bin = pd.get_dummies(y_true).values
    plt.figure(figsize=(8, 6))
    for i in range(y_probs.shape[1]):
        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])
        auc_score = auc(fpr, tpr)
        label = f"{class_names[i] if class_names else f'Class {i}'} (AUC = {auc_score:.2f})"
        plt.plot(fpr, tpr, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"{set_name} ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    return results_df_multi

evaluate_multiclass_model("Training", model, X_train_pad, y_train, class_names=["Negative", "Neutral", "Positive"])

evaluate_multiclass_model("Validation", model, X_val_pad, y_val, class_names=["Negative", "Neutral", "Positive"])

# model.save("GRU_model1.keras")

from keras.models import load_model

model1 = load_model("GRU_model1.keras")

"""# Observations:

1. Accuracy from training to validation dropped by 8%, indicating slight overfitting. We can play with drop out.
2. Model is struggling to predict class "Neutral". We can try adding more weight to that class.
3. Positive and negative classes have good f1 score of 76%. Can be improved.

# Next steps: Model tuning

1. Reduce Overfitting
- Dropout Increase
- reducing to 1 BiGRU
- kernel_regularizer=l2 to Dense layer

2. Improving Accuracy

- spell check in pre processing
- Attention layer
- class weights

| **Metric / Aspect**                   | **Details**                                                                            |
| ------------------------------------- | -------------------------------------------------------------------------------------- |
| **Model Type**                        | Deep Bidirectional GRU                                                                 |
| **Embedding**                         | Keras Embedding (vocab\_size=10,000, embed\_dim=100)                                   |
| **Model Configuration**               | Embedding â†’ BiGRU(64) â†’ Dropout(0.3) â†’ BiGRU(64) â†’ Dense(64, ReLU) â†’ Dense(3, Softmax) |
| **Training Time**                     | 11 minutes (16 cpus, 64gb ram)                |
| **Training Data Checked**             | Cleaned, tokenized, padded, and validated                                            |
| **Confusion Matrix Built?**           | Yes                                                                                  |
| **F1 Score â€“ Positive Class**         | â‰ˆ 0.83                                                                                 |
| **F1 Score â€“ Negative Class**         | â‰ˆ 0.85                                                                                 |
| **ROC AUC Plotted?**                  | Yes (macro & per-class curves)                                                       |
| **AUC Score**                         | â‰ˆ 0.94 (macro average)                                                                 |
| **Accuracy Computed?**                | Yes                                                                                  |
| **Accuracy**                          | â‰ˆ 80%                                                                                   |
| **Categorical / Numerical Features?** | Not yet (only text)                                                                  |
| **2 Suggested Features to Add**       | 1. Tweet Length<br>2. Emoji or punctuation count                                       |
"""

# from keras.models import load_model

# model = load_model("GRU_model1.keras")

!pip install lime --quiet
from lime.lime_text import LimeTextExplainer

# label encoder/decoder
class_names = ['Negative', 'Neutral', 'Positive']

def predict_proba(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')
    return model.predict(padded)

explainer = LimeTextExplainer(class_names=class_names)

i = 10
text_sample = X_test.iloc[i]
exp = explainer.explain_instance(text_sample, predict_proba, num_features=10)
exp.show_in_notebook(text=True)

from collections import defaultdict
global_weights = defaultdict(float)

# Sample
for i in range(30):
    exp = explainer.explain_instance(X_test.iloc[i], predict_proba, num_features=10)
    for word, weight in exp.as_list():
        global_weights[word] += abs(weight)

#
global_df = pd.DataFrame(global_weights.items(), columns=['word', 'importance']).sort_values(by='importance', ascending=False)

# top influential words
global_df.head(20)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
top_words = global_df.head(20)
plt.barh(top_words['word'][::-1], top_words['importance'][::-1])
plt.xlabel('Aggregated LIME Importance')
plt.title('Global Word Importance via LIME')
plt.tight_layout()
plt.show()

# from keras.models import load_model

# model = load_model("GRU_model1.keras")

"""# Model Tuning 1.

1.combined textual data (Processed_Tweet) with engineered features (Emoji_Count, Slang_Count, Abbr_Count).

2. three stacked bidirectional GRU

3. Regularization for Overfitting - Dropout after every GRU and dense layer, L2 regularization in GRU and Dense layers.

4.  EarlyStopping - Stops training when val_loss doesnâ€™t improve for 3 epochs, ReduceLROnPlateau - Reduces learning rate when validation loss plateaus.

"""

# Loading data
sampled_df=pd.read_csv("Twitter_sentiment_data/Full_processed_tweets.csv")

sampled_df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Features
texts = sampled_df['Processed_Tweet'].astype(str)
extra_features = sampled_df[['Emoji_Count', 'Slang_Count', 'Abbr_Count']].values

# Target
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(sampled_df['Sentiment'])

"""# Data splitting"""

# Train vs Temp (Val+Test)
X_text_train, X_text_temp, X_extra_train, X_extra_temp, y_train, y_temp = train_test_split(
    texts, extra_features, labels, test_size=0.2, stratify=labels, random_state=42)

# Validation + Test (each 10%)
X_text_val, X_text_test, X_extra_val, X_extra_test, y_val, y_test = train_test_split(
    X_text_temp, X_extra_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

X_text_test.shape

"""Tokenization and padding"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize on training text only
vocab_size = 10000
maxlen = 100 # hyperparametr

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(X_text_train)

X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_train), maxlen=maxlen, padding='post', truncating='post')
X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_val), maxlen=maxlen, padding='post', truncating='post')
X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_test), maxlen=maxlen, padding='post', truncating='post')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_extra_train_scaled = scaler.fit_transform(X_extra_train)
X_extra_val_scaled = scaler.transform(X_extra_val)
X_extra_test_scaled = scaler.transform(X_extra_test)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense, Dropout, Concatenate
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Input layers
text_input = Input(shape=(X_train_pad.shape[1],), name='text_input')
extra_input = Input(shape=(X_extra_train_scaled.shape[1],), name='extra_input')

# Text branch with 2 stacked GRUs
x = Embedding(input_dim=10000, output_dim=100)(text_input)

x = Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(1e-3)))(x)
x = Dropout(0.3)(x)

x = Bidirectional(GRU(64, return_sequences=False, kernel_regularizer=l2(1e-3)))(x)
x = Dropout(0.3)(x)



combined = Concatenate()([x, extra_input])
x = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(combined)
x = Dropout(0.3)(x)

# Output layer
output = Dense(3, activation='softmax')(x)

# Build and compile
model = Model(inputs=[text_input, extra_input], outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

#
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.3,
    patience=2,
    min_lr=1e-5,
    verbose=1
)


history = model.fit(
    [X_train_pad, X_extra_train_scaled], y_train,
    validation_data=([X_val_pad, X_extra_val_scaled], y_val),
    epochs=15,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)



from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Class names
class_names = ["Negative", "Neutral", "Positive"]

# Predict probabilities
y_val_probs = model.predict([X_val_pad, X_extra_val_scaled])
y_val_pred = np.argmax(y_val_probs, axis=1)

# Accuracy
acc = accuracy_score(y_val, y_val_pred)
print(f" Accuracy: {acc:.4f}")

# Classification Report
print("\n Classification Report:")
print(classification_report(y_val, y_val_pred, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title(" Confusion Matrix")
plt.show()

#  ROC-AUC Score (macro-averaged for multiclass)
y_val_bin = label_binarize(y_val, classes=[0, 1, 2])
roc_auc = roc_auc_score(y_val_bin, y_val_probs, average='macro', multi_class='ovr')
print(f"\n ROC-AUC Score (macro): {roc_auc:.4f}")

#  ROC Curve (macro style)
fpr = {}
tpr = {}
roc_auc_dict = {}
for i in range(len(class_names)):
    fpr[i], tpr[i], _ = roc_curve(y_val_bin[:, i], y_val_probs[:, i])
    roc_auc_dict[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
for i in range(len(class_names)):
    plt.plot(fpr[i], tpr[i], label=f"{class_names[i]} (AUC = {roc_auc_dict[i]:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(" ROC Curves (One-vs-Rest)")
plt.legend()
plt.grid()
plt.show()

"""No improvement in accuracy.

# Model Tuning 2

|
| Recurrent layers   | Reduced from 3 to 2 GRU layers   
|
| Regularization     | Dropout 0.4, L2 (1e-2), BatchNorm  
|
| Learning Rate      | Reduced to `3e-4`        
|
| Class Balancing    | Added `class_weight=balanced`   
|
| Embedding Dropout  | Prevents early layer overfitting  
|
| Simpler Dense Head | One Dense layer to reduce complexity |
"""

pip install vaderSentiment better-profanity

pip install tqdm tqdm-joblib

sampled_df =pd.read_csv("Full_processed_tweets.csv")
sampled_df .head()

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from better_profanity import profanity
from joblib import Parallel, delayed
from tqdm import tqdm
from tqdm_joblib import tqdm_joblib
import pandas as pd
import multiprocessing

# analyzers
analyzer = SentimentIntensityAnalyzer()
profanity.load_censor_words()

# Lexicon thresholds
pos_words = {word for word, score in analyzer.lexicon.items() if score > 1.5}
neg_words = {word for word, score in analyzer.lexicon.items() if score < -1.5}

#
def extract_features_joblib(text):
    if not isinstance(text, str):
        return [0, 0, 0]

    words = text.lower().split()
    pos_count = sum(1 for word in words if word in pos_words)
    neg_count = sum(1 for word in words if word in neg_words)
    has_profanity = int(profanity.contains_profanity(text))

    return [pos_count, neg_count, has_profanity]

#
num_cores = multiprocessing.cpu_count()


with tqdm_joblib(tqdm(desc="Extracting features", total=len(sampled_df))):
    results = Parallel(n_jobs=num_cores)(
        delayed(extract_features_joblib)(text) for text in sampled_df['Processed_Tweet']
    )


feature_df = pd.DataFrame(results, columns=['Positive_Word_Count', 'Negative_Word_Count', 'Profanity_Presence'])
sampled_df = pd.concat([sampled_df, feature_df], axis=1)

sampled_df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Text and label
texts = sampled_df['Processed_Tweet'].astype(str)
labels = sampled_df['Sentiment']

# Numeric features
numerical_cols = ['Emoji_Count', 'Slang_Count', 'Abbr_Count', 'Positive_Word_Count', 'Negative_Word_Count', 'Profanity_Presence']
extra_features = sampled_df[numerical_cols].values

# Encode target
le = LabelEncoder()
y = le.fit_transform(labels)

# Train-val-test split
X_text_train, X_text_temp, X_extra_train, X_extra_temp, y_train, y_temp = train_test_split(
    texts, extra_features, y, test_size=0.2, stratify=y, random_state=42)

X_text_val, X_text_test, X_extra_val, X_extra_test, y_val, y_test = train_test_split(
    X_text_temp, X_extra_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Tokenize
vocab_size = 10000
maxlen = 100

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(X_text_train)

X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_train), maxlen=maxlen, padding='post')
X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_val), maxlen=maxlen, padding='post')
X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_text_test), maxlen=maxlen, padding='post')

# Normalize extra features
scaler = StandardScaler()
X_extra_train_scaled = scaler.fit_transform(X_extra_train)
X_extra_val_scaled = scaler.transform(X_extra_val)
X_extra_test_scaled = scaler.transform(X_extra_test)



from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense, Dropout, Concatenate, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Inputs
text_input = Input(shape=(maxlen,), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=80)(text_input)
x = Dropout(0.3)(x)

# 2-layer BiGRU with moderate regularization
x = Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(1e-3)))(x)
x = Dropout(0.3)(x)
x = Bidirectional(GRU(64, return_sequences=False, kernel_regularizer=l2(1e-3)))(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)

# Extra input branch
extra_input = Input(shape=(X_extra_train_scaled.shape[1],), name='extra_input')
y = BatchNormalization()(extra_input)

# Combine
combined = Concatenate()([x, y])
z = Dense(64, activation='relu', kernel_regularizer=l2(1e-3))(combined)
z = BatchNormalization()(z)
z = Dropout(0.3)(z)

output = Dense(3, activation='softmax')(z)

# Final model
model = Model(inputs=[text_input, extra_input], outputs=output)

model.compile(
    optimizer=Adam(learning_rate=3e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-5)
]

history = model.fit(
    [X_train_pad, X_extra_train_scaled], y_train,
    validation_data=([X_val_pad, X_extra_val_scaled], y_val),
    epochs=25,
    batch_size=128,
    class_weight=class_weights,
    callbacks=callbacks,
    verbose=1
)

from sklearn.metrics import classification_report, accuracy_score

y_test_probs = model.predict([X_test_pad, X_extra_test_scaled])
y_test_pred = y_test_probs.argmax(axis=1)

print(" Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("\n Classification Report:\n", classification_report(y_test, y_test_pred, target_names=le.classes_))

from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

y_test_bin = label_binarize(y_test, classes=[0, 1, 2])  # Assuming 3 classes

roc_auc = roc_auc_score(y_test_bin, y_test_probs, average='macro', multi_class='ovr')

print(f"\n ROC-AUC Score (macro-average): {roc_auc:.4f}")

# ROC Curves for each class
fpr = {}
tpr = {}
roc_auc_individual = {}

for i in range(len(le.classes_)):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_probs[:, i])
    roc_auc_individual[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
for i, label in enumerate(le.classes_):
    plt.plot(fpr[i], tpr[i], label=f"{label} (AUC = {roc_auc_individual[i]:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC-AUC Curve (One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid()
plt.show()

"""Processed_Tweet: Cleaned, normalized, and negation-handled tweet text

Sentiment: Target class (Negative, Neutral, Positive)

Count-based features already included:

Emoji_Count

Slang_Count

Abbr_Count

| Feature Name          | Description                                                |
| --------------------- | ---------------------------------------------------------- |
| `Text_Length`         | Number of characters in the tweet                          |
| `Positive_Word_Count` | Count of strongly positive words from VADER lexicon        |
| `Negative_Word_Count` | Count of strongly negative words from VADER lexicon        |
| `Profanity_Presence`  | Binary flag if profanity detected using `better_profanity` |



| Component           | Setting                        |
| ------------------- | ------------------------------ |
| Embedding Dimension | 80                          |
| GRU Layers          | 2 stacked bidirectional GRUs   |
| GRU Units           | 64                             |
| Dropout             | 0.3 (on embedding, GRU, Dense) |
| L2 Regularization   | 1e-3                           |
| Dense Hidden Layer  | 64 units, ReLU                 |
| Output Layer        | Softmax (3 classes)            |

| Option                                    | Benefit                                 |
| ----------------------------------------- | --------------------------------------- |
| Tune learning rate scheduler              | Smoother convergence (OneCycleLR) |
| Train longer with early stopping          | Might reach slightly better weights     |
| Experiment with transformers              | Stronger text embeddings                |
|
"""

# Model Evaluation on Validation Dataset

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Class names for better readability
class_names = ["Negative", "Neutral", "Positive"]

# Predict on validation data
print("Evaluating Model Tuning 2 on Validation Dataset...")
y_val_probs = model.predict([X_val_pad, X_extra_val_scaled])
y_val_pred = np.argmax(y_val_probs, axis=1)

# 1. Accuracy Score
val_accuracy = accuracy_score(y_val, y_val_pred)
print(f"Validation Accuracy: {val_accuracy:.4f}")

# 2. Classification Report
print("\nClassification Report (Validation):")
print(classification_report(y_val, y_val_pred, target_names=class_names))

# 3. Confusion Matrix
cm = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Validation Confusion Matrix - Model Tuning 2')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 4. ROC-AUC Score (macro-averaged for multiclass)
y_val_bin = label_binarize(y_val, classes=[0, 1, 2])
roc_auc_macro = roc_auc_score(y_val_bin, y_val_probs, average='macro', multi_class='ovr')
print(f"\nROC-AUC Score (macro-average): {roc_auc_macro:.4f}")

# 5. ROC Curves for each class
fpr = {}
tpr = {}
roc_auc_individual = {}

plt.figure(figsize=(10, 8))
for i in range(len(class_names)):
    fpr[i], tpr[i], _ = roc_curve(y_val_bin[:, i], y_val_probs[:, i])
    roc_auc_individual[i] = auc(fpr[i], tpr[i])
    plt.plot(fpr[i], tpr[i],
             label=f'{class_names[i]} (AUC = {roc_auc_individual[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - Validation Set (Model Tuning 2)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# 6. Performance Summary
print("\n" + "="*50)
print("VALIDATION PERFORMANCE SUMMARY - MODEL TUNING 2")
print("="*50)
print(f"Overall Accuracy: {val_accuracy:.4f}")
print(f"Macro-averaged ROC-AUC: {roc_auc_macro:.4f}")
print("\nPer-class ROC-AUC scores:")
for i, class_name in enumerate(class_names):
    print(f"  {class_name}: {roc_auc_individual[i]:.4f}")

# Individual LIME Explanations for Each Class

!pip install lime --quiet
from lime.lime_text import LimeTextExplainer
import numpy as np

# Initialize LIME explainer
class_names = ['Negative', 'Neutral', 'Positive']
explainer = LimeTextExplainer(class_names=class_names)

# Prediction function for LIME
def predict_proba_for_lime(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')

    # Use mean values for numerical features when analyzing text
    dummy_features = np.tile(X_extra_val_scaled.mean(axis=0), (len(texts), 1))

    return model.predict([padded, dummy_features])

# Find one sample from each class in validation set
negative_idx = np.where(y_val == 0)[0][5]  # 5th negative sample
neutral_idx = np.where(y_val == 1)[0][3]   # 3rd neutral sample
positive_idx = np.where(y_val == 2)[0][7]  # 7th positive sample

samples = {
    'Negative': negative_idx,
    'Neutral': neutral_idx,
    'Positive': positive_idx
}

print("="*70)
print("INDIVIDUAL LIME EXPLANATIONS")
print("="*70)

for class_name, idx in samples.items():
    print(f"\n{class_name.upper()} CLASS EXAMPLE:")
    print("-" * 40)

    # Get the text sample
    text_sample = X_text_val.iloc[idx]
    true_label = y_val[idx]

    # Get model prediction
    sample_seq = pad_sequences(tokenizer.texts_to_sequences([text_sample]),
                              maxlen=maxlen, padding='post', truncating='post')
    sample_features = X_extra_val_scaled[idx:idx+1]

    prediction_probs = model.predict([sample_seq, sample_features])[0]
    predicted_class = np.argmax(prediction_probs)
    confidence = prediction_probs[predicted_class]

    print(f"Original Tweet: '{text_sample}'")
    print(f"True Label: {class_names[true_label]}")
    print(f"Predicted Label: {class_names[predicted_class]} (Confidence: {confidence:.3f})")
    print(f"Prediction Probabilities: {dict(zip(class_names, prediction_probs))}")

    # Generate LIME explanation
    print(f"\nGenerating LIME explanation for {class_name} sample...")
    exp = explainer.explain_instance(
        text_sample,
        predict_proba_for_lime,
        num_features=10,
        labels=[predicted_class]  # Focus on predicted class
    )

    # Show explanation in notebook format
    print(f"\nLIME Explanation (showing top contributing words):")
    exp.show_in_notebook(text=True)

    # Also show as list for better readability
    print(f"\nDetailed Word Contributions for {class_names[predicted_class]} prediction:")
    explanation_list = exp.as_list(label=predicted_class)

    positive_words = [(word, weight) for word, weight in explanation_list if weight > 0]
    negative_words = [(word, weight) for word, weight in explanation_list if weight < 0]

    if positive_words:
        print("Words SUPPORTING the prediction:")
        for word, weight in sorted(positive_words, key=lambda x: x[1], reverse=True):
            print(f"  '{word}': +{weight:.3f}")

    if negative_words:
        print("Words OPPOSING the prediction:")
        for word, weight in sorted(negative_words, key=lambda x: x[1]):
            print(f"  '{word}': {weight:.3f}")

    # Show numerical features for this sample
    print(f"\nNumerical Features for this sample:")
    feature_names = ['Emoji_Count', 'Slang_Count', 'Abbr_Count',
                    'Positive_Word_Count', 'Negative_Word_Count', 'Profanity_Presence']

    original_features = sampled_df.iloc[X_text_val.index[idx]][feature_names].values
    for feat_name, orig_val, scaled_val in zip(feature_names, original_features, sample_features[0]):
        print(f"  {feat_name}: {orig_val} (scaled: {scaled_val:.3f})")

    print("\n" + "="*70)

# Summary of LIME insights
print("\nLIME INTERPRETATION INSIGHTS:")
print("="*40)
print("""
ðŸ” NEGATIVE CLASS:
- Look for words that strongly indicate negative sentiment
- Check if negation words (NOT_*) are contributing
- Observe which words are opposing positive classification

ðŸ˜ NEUTRAL CLASS:
- Often characterized by absence of strong sentiment words
- May have balanced positive/negative indicators
- Could show uncertainty in word contributions

ðŸ˜Š POSITIVE CLASS:
- Strong positive sentiment words should dominate
- Look for enthusiasm indicators, positive adjectives
- Minimal contribution from negative sentiment words

ðŸ“Š MODEL BEHAVIOR:
- Higher absolute LIME weights = more influential words
- Positive weights support the predicted class
- Negative weights oppose the predicted class
- LIME helps understand individual prediction decisions
""")

# Global Model Interpretation

# 1. LIME-based Global Text Feature Importance
print("Generating Global Text Feature Importance using LIME...")

!pip install lime --quiet
from lime.lime_text import LimeTextExplainer
from collections import defaultdict
import pandas as pd

# Initialize LIME explainer
class_names = ['Negative', 'Neutral', 'Positive']
explainer = LimeTextExplainer(class_names=class_names)

# Prediction function for LIME
def predict_proba_for_lime(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')

    # Create dummy numerical features (mean values) for LIME text-only analysis
    dummy_features = np.tile(X_extra_val_scaled.mean(axis=0), (len(texts), 1))

    return model.predict([padded, dummy_features])

# Aggregate LIME explanations across multiple samples
global_word_importance = defaultdict(float)
sample_size = 100  # Analyze 100 samples for global interpretation

print(f"Analyzing {sample_size} samples for global word importance...")
for i in range(min(sample_size, len(X_val))):
    try:
        exp = explainer.explain_instance(
            X_text_val.iloc[i],
            predict_proba_for_lime,
            num_features=15
        )

        # Aggregate absolute importance scores
        for word, weight in exp.as_list():
            global_word_importance[word] += abs(weight)
    except:
        continue

# Convert to DataFrame and sort
global_words_df = pd.DataFrame(
    global_word_importance.items(),
    columns=['word', 'aggregated_importance']
).sort_values(by='aggregated_importance', ascending=False)

print("\nTop 20 Most Influential Words Globally:")
print(global_words_df.head(20))

# Visualize top influential words
plt.figure(figsize=(12, 8))
top_words = global_words_df.head(20)
plt.barh(range(len(top_words)), top_words['aggregated_importance'])
plt.yticks(range(len(top_words)), top_words['word'])
plt.xlabel('Aggregated LIME Importance Score')
plt.title('Global Word Importance (Top 20)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 2. Numerical Feature Importance Analysis
print("\n" + "="*50)
print("NUMERICAL FEATURE IMPORTANCE ANALYSIS")
print("="*50)

# Feature statistics
numerical_feature_names = ['Emoji_Count', 'Slang_Count', 'Abbr_Count',
                          'Positive_Word_Count', 'Negative_Word_Count', 'Profanity_Presence']

feature_stats = pd.DataFrame({
    'Feature': numerical_feature_names,
    'Mean': X_extra_val_scaled.mean(axis=0),
    'Std': X_extra_val_scaled.std(axis=0),
    'Min': X_extra_val_scaled.min(axis=0),
    'Max': X_extra_val_scaled.max(axis=0)
})

print("Numerical Feature Statistics (Scaled):")
print(feature_stats)

# Feature correlation with predictions
y_val_probs_class = np.argmax(y_val_probs, axis=1)
feature_correlations = []

for i, feature_name in enumerate(numerical_feature_names):
    correlation = np.corrcoef(X_extra_val_scaled[:, i], y_val_probs_class)[0, 1]
    feature_correlations.append(correlation)

correlation_df = pd.DataFrame({
    'Feature': numerical_feature_names,
    'Correlation_with_Prediction': feature_correlations
})
correlation_df = correlation_df.sort_values('Correlation_with_Prediction', key=abs, ascending=False)

print("\nFeature Correlation with Predictions:")
print(correlation_df)

# Visualize feature correlations
plt.figure(figsize=(10, 6))
plt.barh(correlation_df['Feature'], correlation_df['Correlation_with_Prediction'])
plt.xlabel('Correlation with Prediction')
plt.title('Numerical Feature Correlation with Model Predictions')
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()

# 3. Class-wise Word Analysis
print("\n" + "="*50)
print("CLASS-WISE INTERPRETATION")
print("="*50)

# Analyze predictions by class
class_indices = {
    'Negative': np.where(y_val == 0)[0][:30],
    'Neutral': np.where(y_val == 1)[0][:30],
    'Positive': np.where(y_val == 2)[0][:30]
}

class_word_importance = {}

for class_name, indices in class_indices.items():
    print(f"\nAnalyzing {class_name} class predictions...")
    class_words = defaultdict(float)

    for idx in indices:
        try:
            exp = explainer.explain_instance(
                X_text_val.iloc[idx],
                predict_proba_for_lime,
                num_features=10
            )

            for word, weight in exp.as_list():
                class_words[word] += abs(weight)
        except:
            continue

    # Get top words for this class
    class_top_words = sorted(class_words.items(), key=lambda x: x[1], reverse=True)[:10]
    class_word_importance[class_name] = class_top_words

    print(f"Top 10 words for {class_name} class:")
    for word, importance in class_top_words:
        print(f"  {word}: {importance:.3f}")

# 4. Model Behavior Summary
print("\n" + "="*70)
print("GLOBAL MODEL INTERPRETATION SUMMARY")
print("="*70)

print(f"""
TEXT FEATURES:
- Most influential words globally: {', '.join(global_words_df.head(5)['word'].tolist())}
- Total unique influential words identified: {len(global_words_df)}

NUMERICAL FEATURES:
- Most correlated feature: {correlation_df.iloc[0]['Feature']}
  (correlation: {correlation_df.iloc[0]['Correlation_with_Prediction']:.3f})
- Least correlated feature: {correlation_df.iloc[-1]['Feature']}
  (correlation: {correlation_df.iloc[-1]['Correlation_with_Prediction']:.3f})

CLASS-SPECIFIC PATTERNS:
- Negative class key indicators: {', '.join([word for word, _ in class_word_importance.get('Negative', [])[:3]])}
- Positive class key indicators: {', '.join([word for word, _ in class_word_importance.get('Positive', [])[:3]])}
- Neutral class key indicators: {', '.join([word for word, _ in class_word_importance.get('Neutral', [])[:3]])}

MODEL INSIGHTS:
- The model relies heavily on sentiment-bearing words for classification
- Numerical features provide additional context, with {correlation_df.iloc[0]['Feature']} being most influential
- Different classes show distinct word patterns, indicating good feature learning
""")
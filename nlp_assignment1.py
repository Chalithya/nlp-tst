# -*- coding: utf-8 -*-
"""NLP_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hJhA3Ixt6NiBaysrHr1R1pf2WmKc4wzN
"""

pip install nltk

pip install emoji

pip install swifter

pip install spacy

!pip install negspacy

!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sns
import nltk
from tqdm import tqdm
tqdm.pandas()

from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc

import re
import emoji
import multiprocessing

from multiprocessing import Pool

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os
import warnings


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore')

import tensorflow as tf

!gcloud config set project ml-projects-460616

import os
from google.cloud import storage

from google.cloud import storage
import os

bucket_name="yelp-sentiment-analysis"
csv_file="Sentiment_Data.csv"


local_dir = "./Twitter_sentiment_data"
os.makedirs(local_dir, exist_ok=True)

client = storage.Client()
bucket = client.bucket(bucket_name)


blob = bucket.blob("Sentiment_Data.csv")
local_path = os.path.join(local_dir, "Sentiment_Data.csv")
blob.download_to_filename(local_path)
print(f"Downloaded to {local_path}")

tweets = pd.read_csv("Twitter_sentiment_data/Sentiment_Data.csv", encoding="ISO-8859-1")

tweets.head()

tweets.shape

tweets['Sentiment'].value_counts()

"""# Removing duplicate tweets"""

sum(tweets.duplicated())

tweets.drop_duplicates(inplace=True)
sum(tweets.duplicated())

"""# Removing null tweets"""

tweets.isnull().sum()

# Removing rows where Tweet is null
tweets = tweets.dropna(subset=['Tweet']).reset_index(drop=True)
print("Remaining rows:", len(tweets))

tweets.isnull().sum()

"""# Sampling

I am comnining mild positive to positive and taking 20k from mild positive to be able to separate from others and 10k from strong positive. Similarly for negative class as well. Finally, I am combining them to classify them into only 3 classes which are positive, negative, and neutral. This will be easy to develop. Later on, we will predict on 5 classes for more detailed.
"""

# Sampling
mild_pos = tweets[tweets['Sentiment'] == 'Mild_Pos'].sample(n=20000, random_state=42)
strong_pos = tweets[tweets['Sentiment'] == 'Strong_Pos'].sample(n=15000, random_state=42)

mild_neg = tweets[tweets['Sentiment'] == 'Mild_Neg'].sample(n=20000, random_state=42)
strong_neg = tweets[tweets['Sentiment'] == 'Strong_Neg'].sample(n=15000, random_state=42)

neutral = tweets[tweets['Sentiment'] == 'Neutral'].sample(n=35000, random_state=42)

# combing them
sampled_df = pd.concat([mild_pos, strong_pos, mild_neg, strong_neg, neutral], axis=0).reset_index(drop=True)


print(sampled_df['Sentiment'].value_counts())

sampled_df.head()

# combining them to make  3 classes
def simplify_sentiment(sent):
    if sent in ['Mild_Pos', 'Strong_Pos']:
        return 'Positive'
    elif sent in ['Mild_Neg', 'Strong_Neg']:
        return 'Negative'
    else:
        return 'Neutral'

sampled_df['Sentiment'] = sampled_df['Sentiment'].apply(simplify_sentiment)


print(sampled_df['Sentiment'].value_counts())

sampled_df.head()

"""# Dictionary of Slang words from Github"""

slang_dict  = {
    "$" : " dollar ",
    "â‚¬" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk",
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
    "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
    "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart",
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
    "pto" : "please turn over",
    "qpsa" : "what happens", #"que pasa",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet",
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
    "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously",
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
    "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "yd" : "yard",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}

"""# Abbrevations"""

with open("abbreviations_eng.csv", encoding="ISO-8859-1") as f:
    for i, line in enumerate(f.readlines()[:20]):
        print(f"{i+1}: {line}")

# Loading CSV using semicolon delimiter and no header
abb = pd.read_csv("abbreviations_eng.csv",
                  encoding="ISO-8859-1",
                  delimiter=";",
                  header=None,
                  names=["Index", "abbr", "full"])


abb = abb.drop(columns=["Index"])


print(abb.head())

abbr_dict = dict(
    zip(
        abb['abbr'].astype(str).str.strip(),
        abb['full'].astype(str).str.strip()
    )
)

import re
import emoji
import spacy



def expand_terms(text, mapping_dict):
    count = 0
    for short, expanded in mapping_dict.items():
        pattern = r'\b' + re.escape(short) + r'\b'
        matches = len(re.findall(pattern, text))
        if matches > 0:
            text = re.sub(pattern, expanded, text)
            count += matches
    return text, count

def clean_text(text):
    if not isinstance(text, str):
        return "", 0, 0, 0

    text = text.lower()

    # Emoji handling
    emoji_matches = re.findall(r':[a-z_]+:', emoji.demojize(text))
    emoji_count = len(emoji_matches)
    text = emoji.demojize(text)
    text = text.replace(":", " ").replace("_", " ")

    # Slang handling
    text, slang_count = expand_terms(text, slang_dict)

    # Abbreviation handling
    text, abbr_count = expand_terms(text, abbr_dict)

    # Regex cleaning
    text = re.sub(r"http\S+|www.\S+", "", text)     # URLs
    text = re.sub(r"@\w+", "", text)                # Mentions
    text = re.sub(r"#\w+", "", text)                # Hashtags
    text = re.sub(r"&\w+;", "", text)               # HTML entities
    text = re.sub(r"[^a-z\s]", "", text)            # Non-letter characters
    text = re.sub(r"\s+", " ", text).strip()        # Whitespace



    return text, emoji_count, slang_count, abbr_count

from joblib import Parallel, delayed
from tqdm.notebook import tqdm

def parallel_clean_text_joblib(text_list, func, n_jobs=-1):
    results = Parallel(n_jobs=n_jobs)(
        delayed(func)(text) for text in tqdm(text_list)
    )
    return results

def clean_text_chunk(chunk):
    return chunk.apply(clean_text)

# Apply cleaning in parallel using joblib
results = parallel_clean_text_joblib(sampled_df['Tweet'], clean_text)

# Unpacking the tuple result into new columns in sampled_df
sampled_df['Clean_Tweet'] = [r[0] for r in results]
sampled_df['Emoji_Count'] = [r[1] for r in results]
sampled_df['Slang_Count'] = [r[2] for r in results]
sampled_df['Abbr_Count'] = [r[3] for r in results]

sampled_df.head()

blank_rows = sampled_df[sampled_df['Clean_Tweet'] == ""][['Tweet']]
print("tweets cleaned to empty:")
print(blank_rows)

"""# Removing hashtags and mentions for now."""

# Dropping rows where Clean_Tweet is empty or null
sampled_df = sampled_df[~sampled_df['Clean_Tweet'].isnull()]
sampled_df = sampled_df[sampled_df['Clean_Tweet'].str.strip() != ""]


sampled_df.reset_index(drop=True, inplace=True)

sampled_df.isnull().sum()

# import csv

# sampled_df[['Clean_Tweet', 'Sentiment']].to_csv(
#     "Twitter_sentiment_data/processed_data_without_negation.csv",
#     index=False,
#     encoding='utf-8'
# )



data_nonegate=sampled_df.copy()



"""# Negation handling"""

import spacy
from tqdm.notebook import tqdm

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

def handle_negation(text):
    doc = nlp(text)
    negated = {token.head.i for token in doc if token.dep_ == "neg"}

    return " ".join(
        ["NOT_" + token.text if i in negated else token.text for i, token in enumerate(doc)]
    )


tqdm.pandas(desc="Applying negation to Clean_Tweet")


sampled_df["Negated_Tweet"] = sampled_df["Clean_Tweet"].progress_apply(handle_negation)

sampled_df.isnull().sum()

negate_tweets=sampled_df.copy()

sampled_df.to_csv("Twitter_sentiment_data/processed_tweets_with_negation.csv",index=False)

print("Total Emojis Handled:", sampled_df['Emoji_Count'].sum())
print("Total Slangs Handled:", sampled_df['Slang_Count'].sum())
print("Total Abbreviations Handled:", sampled_df['Abbr_Count'].sum())

# df=pd.read_csv("Twitter_sentiment_data/processed_tweets_with_negation.csv")
# df.head()

df.isnull().sum()

"""# Summary of Preprocessing Steps Before Splitting the Data


1. Dataset Sampling:
   - Selected 105,000 tweets:
     - 20k Mild_Pos, 15k Strong_Pos
     - 20k Mild_Neg, 15k Strong_Neg
     - 35k Neutral

2. Cleaning Pipeline:
   - Lowercased all text
   - Emoji Handling:
     - Converted emojis to text using `emoji.demojize`
     - Counted emoji occurrences
   - Slang & Abbreviation Expansion:
     - Used `slang_dict` and `abbr_dict` to expand informal terms
     - Counted replacements
   - Regex Cleaning:
     - Removed URLs, mentions (@), hashtags (#), HTML entities, special characters
     - Normalized whitespace
     
Total Emojis Handled: 7487

Total Slangs Handled: 7350

Total Abbreviations Handled: 42


3. Negation Handling:
   - Used spaCy to identify negated words
   - Prefixed negated tokens with `NOT_`

4. Post-Cleaning Fixes:
   - Detected and removed ~2,100 tweets that became empty after cleaning (which are tweets with hastags and mentions)
   - Final dataset contains only non-empty, clean tweets

"""

sampled_df=pd.read_csv("Twitter_sentiment_data/processed_tweets_with_negation.csv")

"""# Label Encoding Sentiment column

Negative - 0
Neutral - 1
Positive - 2

"""

from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
sampled_df['Sentiment'] = label_encoder.fit_transform(sampled_df['Sentiment'])


print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

sampled_df.head()

"""# Splitting Data."""

from sklearn.model_selection import train_test_split

X = sampled_df['Negated_Tweet']
y = sampled_df['Sentiment']

# Splitting into train (70%), validation (15%), and test (15%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)


print("Training set size   :", len(X_train))
print("Validation set size :", len(X_val))
print("Test set size       :", len(X_test))

"""# Tokenization and padding"""

# initialize Tokenizer
vocab_size = 10000
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")

# Fittin on training data only
tokenizer.fit_on_texts(X_train)

#Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Determine max length and pad all sequences
maxlen = max([len(seq) for seq in X_train_seq])
X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post', truncating='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post', truncating='post')


print("X_train_pad shape:", X_train_pad.shape)
print("X_val_pad shape  :", X_val_pad.shape)
print("X_test_pad shape :", X_test_pad.shape)

X_train_pad

"""# Deep Bi-directional GRU - Model 1"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout

# defining model
embedding_dim = 100
hidden_units = 64

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(maxlen,)),

    # First Bidirectional GRU layer (returns sequences to stack next layer)
    Bidirectional(GRU(units=hidden_units, return_sequences=True)),
    Dropout(0.3),

    # Second Bidirectional GRU layer
    Bidirectional(GRU(units=hidden_units, return_sequences=False)),
    Dropout(0.3),

    # Dense layers
    Dense(64, activation='relu'),
    Dropout(0.3),

    # Output layer
    Dense(3, activation='softmax')  # for Positive, Neutral, Negative
])


model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

"""# Model Training"""

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)


history = model.fit(
    X_train_pad, y_train,
    epochs=10,
    batch_size=128,
    validation_data=(X_val_pad, y_val),
    callbacks=[early_stop, checkpoint],
    verbose=1
)

from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc


# results dataframe
results_df_multi = pd.DataFrame(columns=["Set", "Accuracy", "F1_macro", "ROC_AUC_macro"])

def evaluate_multiclass_model(set_name, model, X_pad, y_true, class_names=None):
    global results_df_multi

    # Predict probabilities and predicted classes
    y_probs = model.predict(X_pad)
    y_pred = np.argmax(y_probs, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="macro")
    roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovo', average='macro')

    # Update results dataframe
    results_df_multi = pd.concat([
        results_df_multi,
        pd.DataFrame([{
            "Set": set_name,
            "Accuracy": accuracy,
            "F1_macro": f1,
            "ROC_AUC_macro": roc_auc
        }])
    ], ignore_index=True)

    # classification report
    print(f"Classification Report ({set_name}):\n")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # Confusion matrix
    print(f"Confusion Matrix ({set_name}):")
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title(f"{set_name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve (One-vs-rest)
    print(f"ROC-AUC Curve ({set_name}):")
    y_true_bin = pd.get_dummies(y_true).values
    plt.figure(figsize=(8, 6))
    for i in range(y_probs.shape[1]):
        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])
        auc_score = auc(fpr, tpr)
        label = f"{class_names[i] if class_names else f'Class {i}'} (AUC = {auc_score:.2f})"
        plt.plot(fpr, tpr, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"{set_name} ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

    return results_df_multi

evaluate_multiclass_model("Training", model, X_train_pad, y_train, class_names=["Negative", "Neutral", "Positive"])

evaluate_multiclass_model("Validation", model, X_val_pad, y_val, class_names=["Negative", "Neutral", "Positive"])

# model.save("GRU_model1.keras")

"""# Observations:

1. Accuracy from training to validation dropped by 8%, indicating slight overfitting. We can play with drop out.
2. Model is struggling to predict class "Neutral". We can try adding more weight to that class.
3. Positive and negative classes have good f1 score of 76%. Can be improved.

# Next steps: Model tuning

1. Reduce Overfitting
- Dropout Increase
- reducing to 1 BiGRU
- kernel_regularizer=l2 to Dense layer

2. Improving Accuracy

- spell check in pre processing
- Attention layer
- class weights

| **Metric / Aspect**                   | **Details**                                                                            |
| ------------------------------------- | -------------------------------------------------------------------------------------- |
| **Model Type**                        | Deep Bidirectional GRU                                                                 |
| **Embedding**                         | Keras Embedding (vocab\_size=10,000, embed\_dim=100)                                   |
| **Model Configuration**               | Embedding â†’ BiGRU(64) â†’ Dropout(0.3) â†’ BiGRU(64) â†’ Dense(64, ReLU) â†’ Dense(3, Softmax) |
| **Training Time**                     | 11 minutes (16 cpus, 64gb ram)                |
| **Training Data Checked**             | Cleaned, tokenized, padded, and validated                                            |
| **Confusion Matrix Built?**           | Yes                                                                                  |
| **F1 Score â€“ Positive Class**         | â‰ˆ 0.83                                                                                 |
| **F1 Score â€“ Negative Class**         | â‰ˆ 0.85                                                                                 |
| **ROC AUC Plotted?**                  | Yes (macro & per-class curves)                                                       |
| **AUC Score**                         | â‰ˆ 0.94 (macro average)                                                                 |
| **Accuracy Computed?**                | Yes                                                                                  |
| **Accuracy**                          | â‰ˆ 80%                                                                                   |
| **Categorical / Numerical Features?** | Not yet (only text)                                                                  |
| **2 Suggested Features to Add**       | 1. Tweet Length<br>2. Emoji or punctuation count                                       |

| **#** | **Tuning Area**           | **Description**                                                                                     |
| ----: | ------------------------- | --------------------------------------------------------------------------------------------------- |
|     1 | **Architecture Tuning**   | Changing model layers, type (e.g., GRU â†’ LSTM), number of layers, or connections                    |
|     2 | **Hyperparameter Tuning** | Adjusting learning rate, batch size, dropout rate, number of GRU units, etc.                        |
|     3 | **Regularization**        | Adding dropout layers, early stopping, or L2 regularization to prevent overfitting                  |
|     4 | **Optimizer Choices**     | Trying optimizers like Adam, RMSprop, SGD that affect training efficiency                           |
|     5 | **Embedding Tuning**      | Changing embedding size, using pretrained vectors (GloVe, FastText), or making embeddings trainable |
|     6 | **Data Preprocessing**    | Better text cleaning, handling emojis, stopwords, or fixing class imbalance                         |
|     7 | **Feature Engineering**   | Creating new useful features (e.g., sentiment scores, tweet length)                                 |
|     8 | **Evaluation Strategy**   | Using stratified splits, cross-validation, adjusting decision thresholds                            |
"""

from keras.models import load_model

model = load_model("GRU_model1.keras")

!pip install lime --quiet
from lime.lime_text import LimeTextExplainer

# Your label encoder/decoder
class_names = ['Negative', 'Neutral', 'Positive']  # adjust as per your dataset

# Create prediction wrapper using tokenizer and model
def predict_proba(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')
    return model.predict(padded)

explainer = LimeTextExplainer(class_names=class_names)

i = 10  # example index
text_sample = X_test.iloc[i]
exp = explainer.explain_instance(text_sample, predict_proba, num_features=10)
exp.show_in_notebook(text=True)

from collections import defaultdict
global_weights = defaultdict(float)

# Sample
for i in range(30):
    exp = explainer.explain_instance(X_test.iloc[i], predict_proba, num_features=10)
    for word, weight in exp.as_list():
        global_weights[word] += abs(weight)

#
global_df = pd.DataFrame(global_weights.items(), columns=['word', 'importance']).sort_values(by='importance', ascending=False)

# top influential words
global_df.head(20)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
top_words = global_df.head(20)
plt.barh(top_words['word'][::-1], top_words['importance'][::-1])
plt.xlabel('Aggregated LIME Importance')
plt.title('Global Word Importance via LIME')
plt.tight_layout()
plt.show()

